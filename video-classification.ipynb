{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-17T17:26:28.396011Z","iopub.execute_input":"2021-12-17T17:26:28.396296Z","iopub.status.idle":"2021-12-17T17:26:28.401044Z","shell.execute_reply.started":"2021-12-17T17:26:28.396254Z","shell.execute_reply":"2021-12-17T17:26:28.400185Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install pafy youtube-dl moviepy","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:26:28.402808Z","iopub.execute_input":"2021-12-17T17:26:28.403114Z","iopub.status.idle":"2021-12-17T17:26:54.529085Z","shell.execute_reply.started":"2021-12-17T17:26:28.403080Z","shell.execute_reply":"2021-12-17T17:26:54.528153Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport math\nimport pafy\n\nimport random\nimport numpy as np\nimport datetime as dt\nimport tensorflow as tf\nfrom moviepy.editor import *\nfrom collections import deque\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:26:54.531978Z","iopub.execute_input":"2021-12-17T17:26:54.532250Z","iopub.status.idle":"2021-12-17T17:27:00.463587Z","shell.execute_reply.started":"2021-12-17T17:26:54.532213Z","shell.execute_reply":"2021-12-17T17:27:00.462715Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"seed_constant = 23\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntf.random.set_seed(seed_constant)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:00.464976Z","iopub.execute_input":"2021-12-17T17:27:00.465231Z","iopub.status.idle":"2021-12-17T17:27:00.472500Z","shell.execute_reply.started":"2021-12-17T17:27:00.465194Z","shell.execute_reply":"2021-12-17T17:27:00.471084Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Step 1: Download and Extract the Dataset\n \nLet us start by downloading the dataset.\n\nThe Dataset we are using is the UCF50 – Action Recognition Dataset.\n\nUCF50 is an action recognition dataset which contains:\n\n  *  50 Action Categories consisting of realistic YouTube videos\n  *  25 Groups of Videos per Action Category\n  *  133 Average Videos per Action Category\n  *  199 Average Number of Frames per Video\n  *  320 Average Frames Width per Video\n  *  240 Average Frames Height per Video\n  *  26 Average Frames Per Seconds per Video","metadata":{}},{"cell_type":"code","source":"#!wget -nc --no-check-certificate https://www.crcv.ucf.edu/data/UCF50.rar","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:00.474386Z","iopub.execute_input":"2021-12-17T17:27:00.474623Z","iopub.status.idle":"2021-12-17T17:27:00.486858Z","shell.execute_reply.started":"2021-12-17T17:27:00.474591Z","shell.execute_reply":"2021-12-17T17:27:00.486226Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Visualize the Data with its Labels\n\nLet us pick some random videos from each class of the dataset and display it, this will give us a good overview of how the dataset looks like.","metadata":{}},{"cell_type":"code","source":"# Create a Matplotlib figure\nplt.figure(figsize = (30, 30))\n\n# Get Names of all classes in UCF50\nall_classes_names = os.listdir('../input/ucf50/UCF50')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:00.489500Z","iopub.execute_input":"2021-12-17T17:27:00.489767Z","iopub.status.idle":"2021-12-17T17:27:00.525666Z","shell.execute_reply.started":"2021-12-17T17:27:00.489732Z","shell.execute_reply":"2021-12-17T17:27:00.524952Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"> mylist = [\"apple\", \"banana\", \"cherry\"]\n> \n> print(random.sample(mylist, k=2)) \n> \n> > *['banana', 'apple']*\n\n\n**random_range**\n* random_range =random.sample(range(len(all_classes_names)), 20)\n\nReturns 20 randomly selected numerical value which lies in this range from the all_classes_names\n\n\n**enumerate(iterable, start=0)**\n\nParameters:\n* Iterable: any object that supports iteration \n* Start: the index value from which the counter is to be started, by default it is 0\n\n**Random.choice(sequence)**\n\nParameters:\n* sequence \tRequired. A sequence like a list, a tuple, a range of numbers etc.\n\nThe choice() method returns a randomly selected element from the specified sequence.\nThe sequence can be a string, a range, a list, a tuple or any other kind ofsequence.\n\n\nThese are the main functions in OpenCV video I/O that we are going to discuss in this blog post:\n\n**cv2.VideoCapture**\n\nCreates a video capture object, which would help stream or display the video.\n    \n**cv2.VideoWriter** \n\nSaves the output video to a directory.\n   \nIn addition, we also discuss other needed functions such as **cv2.imshow()**, **cv2.waitKey()** and the **get()** method which is used to read the video metadata such as frame height, width, fps etc.\n\nThe **vid_capture.read()** method returns a tuple, where the first element is a boolean and the next element is the actual video frame. When the first element is True, it indicates the video stream contains a frame to read. \n\nIf there is a frame to read, you can then use **imshow()** to display the current frame in a window, otherwise exit the loop. \n\n**video_reader.release()**\n\nWhen you call video_reader.release(), then:\n* release software resource\n* release hardware resource\n\n**cv2.putText(image, text, org, font, fontScale, color[,  thickness [, lineType [, bottomLeftOrigin ] ] ] )**\n\nParameters:\n* image : It is the image on which text is to be drawn.\n* text : Text string to be drawn.\n* org : It is the coordinates of the bottom-left corner of the text string in the image. The coordinates are represented as tuples of two values i.e. (X coordinate value, Y coordinate value).\n* font : It denotes the font type. Some of font types are FONT_HERSHEY_SIMPLEX, FONT_HERSHEY_PLAIN, , etc.\n* fontScale : Font scale factor that is multiplied by the font-specific base size.\n* color : It is the color of text string to be drawn. For BGR, we pass a tuple. eg: (255, 0, 0) for blue color.\n* thickness : It is the thickness of the line in px.\n* lineType : This is an optional parameter.It gives the type of the line to be used.\n* bottomLeftOrigin : This is an optional parameter. When it is true, the image data origin is at the bottom-left corner. Otherwise, it is at the top-left corner.","metadata":{}},{"cell_type":"code","source":"# Generate a random sample of images each time the cell runs\nrandom_range =random.sample(range(len(all_classes_names)), 20)\n\n\n# Iterating through all the random samples\nfor counter, random_index in enumerate(random_range, 1):\n    \n    # Getting Class Name using Random Index\n    selected_class_name = all_classes_names[random_index]\n        \n    # Getting a list of all the video files present in a Class Directory\n    video_files_names_list = os.listdir(f'../input/ucf50/UCF50/{selected_class_name}')\n    \n    # Randomly selecting a video file\n    selected_video_file_name = random.choice(video_files_names_list)\n    \n    # Reading the Video File Using the Video Capture\n    video_reader = cv2.VideoCapture(f'../input/ucf50/UCF50/{selected_class_name}/{selected_video_file_name}')\n    \n    # Reading The First Frame of the Video File\n    _, bgr_frame = video_reader.read()\n   \n    # Closing the VideoCapture object and releasing all resources. \n    video_reader.release()\n    \n    # Converting the BGR Frame to RGB Frame  \n    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n    \n    # Adding The Class Name Text on top of the Video Frame.\n    cv2.putText(rgb_frame, selected_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n    \n    # Assigning the Frame to a specific position of a subplot\n    plt.subplot(4, 5, counter)\n    plt.imshow(rgb_frame)\n    plt.axis('off')\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:00.526856Z","iopub.execute_input":"2021-12-17T17:27:00.527166Z","iopub.status.idle":"2021-12-17T17:27:02.945955Z","shell.execute_reply.started":"2021-12-17T17:27:00.527130Z","shell.execute_reply":"2021-12-17T17:27:02.945166Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### **Step 3: Read and Preprocess the Dataset**\nSince we are going to use a classification architecture to train on a video classification dataset, we are going to need to preprocess the dataset first.\n\nNow w constants,\n\n* image_height and image_weight: This is the size we will resize all frames of the video to, we are doing this to avoid unnecessary computation.\n* max_images_per_class: Maximum number of training images allowed for each class.\n* dataset_directory: The path of the directory containing the extracted dataset.\n* classes_list: These are the list of classes we are going to be training on, we are training on following 4 classes, you can feel free to change it.\n    1. tai chi\n    2. Swinging\n    3. Horse Racing\n    4. Walking with a Dog \n\n**Note**: The **image_height**, **image_weight** and **max_images_per_class** constants may be increased for better results, but be warned this will become computationally expensive.","metadata":{}},{"cell_type":"code","source":"image_height, image_width = 64, 64\nmax_images_per_classes = 8000\n\ndataset_directory = '../input/ucf50/UCF50'\nclasses_list = [\"Swing\", \"HorseRace\", \"WalkingWithDog\",\"TaiChi\"]\n\nmodel_output_size = len(classes_list)\n ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:02.946989Z","iopub.execute_input":"2021-12-17T17:27:02.947221Z","iopub.status.idle":"2021-12-17T17:27:02.953073Z","shell.execute_reply.started":"2021-12-17T17:27:02.947191Z","shell.execute_reply":"2021-12-17T17:27:02.952213Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### **Extract, Resize and Normalize Frames** \n\nNow we will create a function that will extract frames from each video while performing other preprocessing operation like resizing and normalizing images.\n\nThis method takes a video file path as input. It then reads the video file frame by frame, resizes each frame, normalizes the resized frame, appends the normalized frame into a list, and then finally returns that list.","metadata":{}},{"cell_type":"code","source":"def frames_extraction(video_path):\n    \n    # Empty list declared to store video frame \n    frame_list = []\n    \n    # Reading the video file using the VideoCapture\n    video_reader = cv2.VideoCapture(video_path)\n    \n    # Iterating through the video frames\n    while True:\n        \n        # Reading a frame from a video file\n        success, frame = video_reader.read()\n        \n        # If the video frame is not successfully read then break the loop\n        if not success:\n            break\n        \n        # Resize the frame to the fixed Dimension\n        resized_frame = cv2.resize(frame, (image_height, image_width))\n        \n        # Now, Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n        normalized_frame = resized_frame / 255\n        \n        # Appending the normalized frame into the frame list \n        frame_list.append(normalized_frame)\n\n    # Closing the video capture object and releasing all the resources\n    video_reader.release()\n\n    # Returning the frames list\n    return frame_list","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:02.954675Z","iopub.execute_input":"2021-12-17T17:27:02.955130Z","iopub.status.idle":"2021-12-17T17:27:02.964597Z","shell.execute_reply.started":"2021-12-17T17:27:02.955087Z","shell.execute_reply":"2021-12-17T17:27:02.963628Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## **Dataset Creation**\n \nNow we will create another function called **create_dataset()**, this function uses the **frame_extraction()** function above and creates our final preprocessed dataset.\n\nHere’s how this function works:\n\n   1.  Iterate through all the classes mentioned in the **classes_list**\n   2.  Now for each class iterate through all the video files present in it.\n   3.  Call the frame_extraction method on each video file.\n   4.  Add the returned frames to a list called temp_features\n   5.  After all videos of a class are processed, randomly select video frames (**equal to            max_images_per_class**) and add them to the list called features.\n   6. Add labels of the selected videos to the **`labels`** list.\n   7. After all videos of all classes are processed then return the features and labels as           NumPy arrays.\n\nSo when you call this function, it returns two lists:\n\n   * **A list of feature vectors**\n   * **A list of its associated labels.**","metadata":{}},{"cell_type":"code","source":"def create_dataset():\n    \n    # Declaring Empty lists to store the Feature and Labels values.\n    temp_feature = []\n    features = []\n    labels = []\n    \n    # Iterating through all the classes present in the classes list\n    for class_index, class_name in enumerate(classes_list):\n        print(f'Extracting data from the Class : {class_name}')\n        \n        # Getting the list of video files present in the specific class name directory\n        files_list = os.listdir(os.path.join(dataset_directory, class_name))\n        \n        # Iterating through all the files present in the files list\n        for file_name in files_list:\n            \n            # Create the video file path \n            video_file_path = os.path.join(dataset_directory, class_name, file_name)\n            \n            # Calling the frame extraction method for every video file path\n            frames = frames_extraction(video_file_path)\n            #print(len(frames))\n            \n            # Appending the frames to the temporary list\n            temp_feature.extend(frames)\n            \n        # Adding randomly selected frames to the feature list\n        features.extend(random.sample(temp_feature, max_images_per_classes ))\n            \n        # Adding fixed number of labels to the labels list\n        labels.extend([class_index]*max_images_per_classes)\n            \n        # Emptying the temp features list so it can be reused for another to store all frames of the next class.\n        temp_feature.clear()\n            \n    # Converting the features and labels lists to numpy arrays\n    features = np.asarray(features)\n    labels = np.array(labels)\n            \n    return features, labels\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:02.965948Z","iopub.execute_input":"2021-12-17T17:27:02.966600Z","iopub.status.idle":"2021-12-17T17:27:02.977965Z","shell.execute_reply.started":"2021-12-17T17:27:02.966530Z","shell.execute_reply":"2021-12-17T17:27:02.977000Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Calling the **create_dataset** method which returns features and labels.","metadata":{}},{"cell_type":"code","source":"features, labels = create_dataset()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:02.981201Z","iopub.execute_input":"2021-12-17T17:27:02.981517Z","iopub.status.idle":"2021-12-17T17:27:39.631622Z","shell.execute_reply.started":"2021-12-17T17:27:02.981480Z","shell.execute_reply":"2021-12-17T17:27:39.630811Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Now we will convert class labels to one hot encoded vectors.","metadata":{}},{"cell_type":"code","source":"# Using keras to categorical method to convert labels into one-hot-encoded vectors\none_hot_encoded_labels = to_categorical(labels)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:39.632827Z","iopub.execute_input":"2021-12-17T17:27:39.633064Z","iopub.status.idle":"2021-12-17T17:27:39.640130Z","shell.execute_reply.started":"2021-12-17T17:27:39.633031Z","shell.execute_reply":"2021-12-17T17:27:39.639271Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Step 4: Split the Data into Train and Test Sets**\n\nNow we have two numpy arrays, one containing all images. The second one contains all class labels in one hot encoded format. Let us split our data to create a training, and a testing set. We must shuffle the data before the split, which we have already done.","metadata":{}},{"cell_type":"code","source":"feature_train, feature_test, label_train, label_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.2, shuffle = True, random_state = seed_constant)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:39.641743Z","iopub.execute_input":"2021-12-17T17:27:39.642077Z","iopub.status.idle":"2021-12-17T17:27:40.624905Z","shell.execute_reply.started":"2021-12-17T17:27:39.642042Z","shell.execute_reply":"2021-12-17T17:27:40.624170Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### **Step 5: Construct the Model**\n\nNow it is time to create our CNN model, for this post, we are creating a simple CNN Classification model with two CNN layers.","metadata":{}},{"cell_type":"code","source":"# Let's create the function that will create our model\ndef create_model():\n    \n    # We will use a sequential model for model construction\n    model = Sequential()\n    \n    # Defining the model Architecture\n    model.add( Conv2D ( filters = 64, kernel_size = ( 3, 3 ), activation = 'relu', input_shape = ( image_height, image_width, 3 )))\n    model.add( Conv2D ( filters = 64, kernel_size = ( 3, 3 ), activation = 'relu' ))\n    model.add( BatchNormalization())\n    model.add( MaxPooling2D ( pool_size = ( 2, 2 )))\n    model.add( GlobalAveragePooling2D())\n    model.add( Dense( 256, activation = 'relu'))\n    model.add( BatchNormalization())\n    model.add( Dense( model_output_size, activation = 'softmax'))\n    \n    # Print the model Summary\n    model.summary()\n    \n    return model\n\n# Calling the create model method \nmodel = create_model()\n\nprint(f'Model Created Successfully!')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:27:40.626078Z","iopub.execute_input":"2021-12-17T17:27:40.627448Z","iopub.status.idle":"2021-12-17T17:27:42.952655Z","shell.execute_reply.started":"2021-12-17T17:27:40.627407Z","shell.execute_reply":"2021-12-17T17:27:42.951956Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**Check Model’s Structure :**\n\nUsing the **plot_model** function, we can check the structure of the final model. This is really helpful when we are creating a complex network, and you want to make sure we have constructed the network correctly.","metadata":{}},{"cell_type":"code","source":"plot_model( model, to_file = 'model_structure_plot.png', show_shapes = True, show_layer_names = True)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:28:21.775058Z","iopub.execute_input":"2021-12-17T17:28:21.775598Z","iopub.status.idle":"2021-12-17T17:28:22.571067Z","shell.execute_reply.started":"2021-12-17T17:28:21.775558Z","shell.execute_reply":"2021-12-17T17:28:22.570337Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Step 6: Compile and Train the Model**\n\n\nNow let us start the training. Before we do that, we also need to compile the model.","metadata":{}},{"cell_type":"code","source":"# Adding early stopping Callback\nearly_stopping_callback  = EarlyStopping( monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True )\n\n# Adding loss value, optimizer and metrics value to the model\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n\n# Start Training \nmodel_training_history = model.fit( x = feature_train, y = label_train, epochs = 50, batch_size = 4, shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:38:45.300879Z","iopub.execute_input":"2021-12-17T17:38:45.301134Z","iopub.status.idle":"2021-12-17T17:55:10.354463Z","shell.execute_reply.started":"2021-12-17T17:38:45.301105Z","shell.execute_reply":"2021-12-17T17:55:10.353651Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Evaluating Your Trained Model**\n\nEvaluate your trained model on the feature’s and label’s test sets.","metadata":{}},{"cell_type":"code","source":"model_evaluation_history = model.evaluate(feature_test, label_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T18:00:29.127849Z","iopub.execute_input":"2021-12-17T18:00:29.128655Z","iopub.status.idle":"2021-12-17T18:00:30.791716Z","shell.execute_reply.started":"2021-12-17T18:00:29.128614Z","shell.execute_reply":"2021-12-17T18:00:30.790993Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Save Your Model**\n\nYou should now save your model for future runs.","metadata":{}},{"cell_type":"code","source":"# Creating a useful name for our model, incase you're saving multiple models (OPTIONAL)","metadata":{},"execution_count":null,"outputs":[]}]}